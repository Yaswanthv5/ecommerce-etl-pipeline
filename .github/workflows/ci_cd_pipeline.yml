name: Data Pipeline CI/CD

on:
  push:
    branches:
      - main
    paths:
      - 'databricks_notebooks/**'
      - 'dbt_project/**'
      - 'dags/**'
      - 'tests/**'
  pull_request:
    branches:
      - main
    paths:
      - 'databricks_notebooks/**'
      - 'dbt_project/**'
      - 'dags/**'
      - 'tests/**'
      - 'airflow_env/**'

env:
  GCP_PROJECT_ID: batch-processing-de # Replace with your GCP Project ID
  GCS_DAGS_BUCKET: batch-processing-de_dags/ # Create a GCS bucket for Airflow DAGs
  GCS_SPARK_SCRIPTS_BUCKET: gs://batch-processing-de_databricks/ # Create a GCS bucket for Spark scripts/jars
  # You'll need to set up GCP_SA_KEY as a GitHub Secret
  # Go to your GitHub repo settings -> Secrets -> Actions -> New repository secret
  # Name: GCP_SA_KEY, Value: content of your GCP service account JSON key file (base64 encoded if issues, but usually plain JSON works)

jobs:
  build_and_test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt # Include pytest, pyspark, google-cloud-storage, dbt-bigquery

      - name: Authenticate GCP Service Account
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Run Pytest for Spark
        run: |
          pytest tests/Spark/

      #- name: Configure dbt profiles.yml
      #  env:
      #    GCP_SA_KEY_FILE: ${{ runner.tempDir }}/gcp_sa_key.json
      #  run: |
      #   echo '${{ secrets.GCP_SA_KEY }}' > "${GCP_SA_KEY_FILE}"
      #    mkdir -p ~/.dbt/
      #    cat <<EOF > ~/.dbt/profiles.yml
      #    ecom_dwh:
      #      outputs:
      #        dev:
      #          type: bigquery
      #          location: us
      #          priority:interactive
      #          job_creation_timeout_seconds: 300
      #          client_id: 107951547706833706253
      #          method: service-account
      #          project: ${{ env.GCP_PROJECT_ID }}
      #          dataset: ecom_dwh
      #          threads: 1
      #          keyfile: ${GCP_SA_KEY_FILE}
      #      target: dev
      #    EOF

      #- name: Install dbt dependencies
      #  run: |
      #    cd dbt_project
      #    dbt deps

      #- name: Run dbt debug and parse (CI - syntax and basic config check)
      #  run: |
      #    cd dbt_project
      #    dbt debug --profile ecom_dwh --target dev
      #    dbt parse --profile ecom_dwh --target dev

  deploy:
    runs-on: ubuntu-latest
    needs: build_and_test
    if: github.ref == 'refs/heads/main' # Deploy only on merge to main
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'

      - name: Setup Databricks
        uses: databricks/setup-cli@main

      - name: Deploy notebooks to Databricks
        run: |
          databricks workspace import-dir ./databricks_notebooks /Users/7ca1009c-bcbe-4fd4-81b8-e475f6b731be/ --overwrite
        env:
          DATABRICKS_HOST: ${{ secrets.DB_HOST }}
          DATABRICKS_CLIENT_ID: ${{ secrets.DATABRICKS_CLIENT_ID}}
          DATABRICKS_CLIENT_SECRET: ${{ secrets.DATABRICKS }}

      - uses: actions/checkout@v4

      - name: Authenticate GCP Service Account
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Upload Airflow DAGs to GCS
        run: |
          gsutil rsync -r dags/ gs://${{ env.GCS_DAGS_BUCKET }}/dags/


      #- name: Configure dbt profiles.yml for deployment
      #  env:
      #    GCP_SA_KEY_FILE: ${{ runner.tempDir }}/gcp_sa_key.json
      #  run: |
      #    echo '${{ secrets.GCP_SA_KEY }}' > "${GCP_SA_KEY_FILE}"
      #    mkdir -p ~/.dbt/
      #    cat <<EOF > ~/.dbt/profiles.yml
      #    ecom_dwh:
      #      target: prod # You might have a separate 'prod' target with different dataset
      #      outputs:
      #        prod:
      #          type: bigquery
      #          method: service-account
      #          project: ${{ env.GCP_PROJECT_ID }}
      #          dataset: ecom_data_warehouse # Or ecom_data_warehouse_prod
      #         threads: 4
      #          keyfile: ${GCP_SA_KEY_FILE}
      #   EOF

      #- name: Run dbt build (run and test) on main merge
      #  run: |
      #    cd dbt_project
      #    dbt build --profile ecom_dwh --target prod # Use 'build' to run and test