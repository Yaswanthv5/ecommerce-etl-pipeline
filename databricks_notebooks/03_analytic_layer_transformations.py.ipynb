{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark_scripts/03_analytic_layer_transformations.py\n",
    "# This script reads from DLT-generated Silver Delta tables, applies\n",
    "# analytic transformations including CDC/SCD logic, and writes\n",
    "# the final analytical tables to GCS (S3 equivalent) as Parquet.\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, date_format, to_date, sha2, concat_ws, when, max, min\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def process_analytic_layer(\n",
    "    spark: SparkSession,\n",
    "    dlt_storage_path: str,      # Base path of DLT pipeline storage (contains /tables)\n",
    "    analytic_layer_gcs_path: str # Base path for writing final analytical tables to GCS\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads silver Delta tables, applies analytic transformations and CDC/SCD logic,\n",
    "    and writes to the analytic layer in GCS (Parquet).\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The active SparkSession.\n",
    "        dlt_storage_path (str): Base GCS path to the DLT pipeline's storage location.\n",
    "        analytic_layer_gcs_path (str): Base GCS path to write the final analytical Parquet tables.\n",
    "    \"\"\"\n",
    "    print(f\"Starting analytic layer transformations.\")\n",
    "    print(f\"Reading from DLT storage: {dlt_storage_path}\")\n",
    "    print(f\"Writing to analytic layer GCS: {analytic_layer_gcs_path}\")\n",
    "\n",
    "    # Define paths to DLT Silver tables\n",
    "    silver_sales_path = f\"{dlt_storage_path}/tables/silver_sales\"\n",
    "    silver_products_path = f\"{dlt_storage_path}/tables/silver_products\"\n",
    "    silver_customers_path = f\"{dlt_storage_path}/tables/silver_customers\"\n",
    "\n",
    "    # Define paths for final analytic tables in GCS\n",
    "    fact_sales_output_path = f\"{analytic_layer_gcs_path}/fact_daily_sales\"\n",
    "    dim_products_output_path = f\"{analytic_layer_gcs_path}/dim_products\"\n",
    "    dim_customers_output_path = f\"{analytic_layer_gcs_path}/dim_customers\"\n",
    "\n",
    "    # --- 1. Process Fact Table (fact_daily_sales) ---\n",
    "    print(\"\\nProcessing fact_daily_sales...\")\n",
    "    try:\n",
    "        # Read the latest state of silver_sales (which is already incrementally updated by DLT)\n",
    "        df_silver_sales = spark.read.format(\"delta\").load(silver_sales_path)\n",
    "\n",
    "        if df_silver_sales.isEmpty():\n",
    "            print(\"No new data in silver_sales. Skipping fact_daily_sales processing.\")\n",
    "        else:\n",
    "            # Simple transformation for fact table: select relevant columns for analysis\n",
    "            df_fact_sales = df_silver_sales.select(\n",
    "                col(\"order_id\"),\n",
    "                col(\"customer_id\"),\n",
    "                col(\"order_date\"), # Use this for partitioning\n",
    "                col(\"order_status\"),\n",
    "                col(\"original_total_amount\"),\n",
    "                col(\"calculated_order_total\"),\n",
    "                col(\"total_products_in_order\"),\n",
    "                col(\"total_quantity_in_order\"),\n",
    "                col(\"silver_processed_timestamp\").alias(\"audit_load_timestamp\") # Audit column\n",
    "            )\n",
    "\n",
    "            # Write to GCS as Parquet, partitioned by order_date\n",
    "            # Using 'append' mode because facts are typically append-only.\n",
    "            # Airflow will then pick up new partitions to load to BigQuery.\n",
    "            df_fact_sales.write.mode(\"append\").partitionBy(\"order_date\").parquet(fact_sales_output_path)\n",
    "            print(f\"fact_daily_sales written to {fact_sales_output_path} (appended, partitioned by order_date).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing fact_daily_sales: {e}\")\n",
    "        # Re-raise to fail the job if critical\n",
    "\n",
    "    # --- 2. Process Dimension Table (dim_products) - SCD Type 2 ---\n",
    "    print(\"\\nProcessing dim_products (SCD Type 2)...\")\n",
    "    try:\n",
    "        df_silver_products = spark.read.format(\"delta\").load(silver_products_path)\n",
    "\n",
    "        # Define schema for the target dim_products table (if it doesn't exist)\n",
    "        # This will ensure consistent schema during merge operations\n",
    "        # Add SCD columns: effective_start_date, effective_end_date, current_flag, hash_value\n",
    "        df_silver_products_with_hash = df_silver_products.withColumn(\n",
    "            \"product_hash\",\n",
    "            sha2(concat_ws(\"||\",\n",
    "                col(\"product_name\"),\n",
    "                col(\"product_category\"),\n",
    "                col(\"product_price\")\n",
    "            ), 256)\n",
    "        ).select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"product_category\",\n",
    "            \"product_price\",\n",
    "            \"product_hash\",\n",
    "            current_timestamp().alias(\"effective_start_date\") # New records start now\n",
    "        )\n",
    "\n",
    "        # Check if the target dimension table exists in GCS\n",
    "        # If it's the first run, create it. Otherwise, perform the SCD merge.\n",
    "        try:\n",
    "            df_current_dim_products = spark.read.parquet(dim_products_output_path)\n",
    "            print(\"Existing dim_products found. Performing SCD Type 2 merge.\")\n",
    "\n",
    "            # Identify new records and changed records\n",
    "            # New records: in silver but not in current_dim (based on product_id)\n",
    "            # Changed records: in both, but hash value differs\n",
    "\n",
    "            # Find records that are NOT in the current dimension (new products)\n",
    "            new_products = df_silver_products_with_hash.alias(\"new_p\").join(\n",
    "                df_current_dim_products.filter(col(\"current_flag\") == True).alias(\"current_p\"), # Only compare with current active records\n",
    "                col(\"new_p.product_id\") == col(\"current_p.product_id\"),\n",
    "                \"left_anti\" # Get rows from new_p that are not in current_p\n",
    "            )\n",
    "\n",
    "            # Find changed records (existing product_id but changed attributes/hash)\n",
    "            changed_products = df_silver_products_with_hash.alias(\"new_p\").join(\n",
    "                df_current_dim_products.filter(col(\"current_flag\") == True).alias(\"current_p\"),\n",
    "                col(\"new_p.product_id\") == col(\"current_p.product_id\"),\n",
    "                \"inner\"\n",
    "            ).where(col(\"new_p.product_hash\") != col(\"current_p.product_hash\")) \\\n",
    "            .select(col(\"new_p.*\")) # Select all columns from the new product for the new version\n",
    "\n",
    "\n",
    "            # Get unchanged records (existing product_id and same hash) to carry them forward\n",
    "            unchanged_products = df_silver_products_with_hash.alias(\"new_p\").join(\n",
    "                df_current_dim_products.filter(col(\"current_flag\") == True).alias(\"current_p\"),\n",
    "                (col(\"new_p.product_id\") == col(\"current_p.product_id\")) & (col(\"new_p.product_hash\") == col(\"current_p.product_hash\")),\n",
    "                \"inner\"\n",
    "            ).select(col(\"current_p.*\")) # Keep the existing current record as is\n",
    "\n",
    "\n",
    "            # Mark old versions of changed products as expired\n",
    "            expired_products = df_current_dim_products.alias(\"current_p\").join(\n",
    "                changed_products.alias(\"changed_p\"),\n",
    "                (col(\"current_p.product_id\") == col(\"changed_p.product_id\")) & (col(\"current_p.current_flag\") == True),\n",
    "                \"inner\"\n",
    "            ).withColumn(\"effective_end_date\", col(\"changed_p.effective_start_date\") - expr(\"INTERVAL 1 DAY\")) \\\n",
    "            .withColumn(\"current_flag\", lit(False)) \\\n",
    "            .select(df_current_dim_products.columns) # Select original columns to maintain schema\n",
    "\n",
    "\n",
    "            # Combine all pieces for the new dimension table state\n",
    "            # 1. Old records that are now expired\n",
    "            # 2. Old records that are unchanged and still current\n",
    "            # 3. New records (either truly new, or new versions of changed records)\n",
    "\n",
    "            # Get records that were current but are NOT in the changed_products set (these remain active or unchanged)\n",
    "            current_active_unchanged = df_current_dim_products.filter(col(\"current_flag\") == True).alias(\"c_curr\") \\\n",
    "                .join(changed_products.alias(\"c_chg\"), col(\"c_curr.product_id\") == col(\"c_chg.product_id\"), \"left_anti\") \\\n",
    "                .select(col(\"c_curr.*\"))\n",
    "\n",
    "            # Combine all previous non-current records (historicals)\n",
    "            historical_records = df_current_dim_products.filter(col(\"current_flag\") == False)\n",
    "\n",
    "            # New versions of changed products and entirely new products\n",
    "            new_and_updated_versions = df_silver_products_with_hash.withColumn(\"effective_end_date\", lit(None).cast(DateType())) \\\n",
    "                                                            .withColumn(\"current_flag\", lit(True).cast(BooleanType())) \\\n",
    "                                                            .select(df_current_dim_products.columns)\n",
    "\n",
    "\n",
    "            df_final_dim_products = historical_records.union(expired_products) \\\n",
    "                                                    .union(current_active_unchanged) \\\n",
    "                                                    .union(new_and_updated_versions)\n",
    "\n",
    "\n",
    "            df_final_dim_products.write.mode(\"overwrite\").parquet(dim_products_output_path)\n",
    "            print(f\"dim_products (SCD Type 2) updated to {dim_products_output_path}.\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            # If table doesn't exist, this is the first run\n",
    "            if \"Path does not exist\" in str(e) or \"IllegalArgumentException: Path does not exist\" in str(e):\n",
    "                print(\"dim_products not found. Initial load for SCD Type 2.\")\n",
    "                df_initial_dim_products = df_silver_products_with_hash.withColumn(\"effective_end_date\", lit(None).cast(DateType())) \\\n",
    "                                                                    .withColumn(\"current_flag\", lit(True).cast(BooleanType())) \\\n",
    "                                                                    .select(\n",
    "                                                                        \"product_id\",\n",
    "                                                                        \"product_name\",\n",
    "                                                                        \"product_category\",\n",
    "                                                                        \"product_price\",\n",
    "                                                                        \"product_hash\",\n",
    "                                                                        \"effective_start_date\",\n",
    "                                                                        \"effective_end_date\",\n",
    "                                                                        \"current_flag\"\n",
    "                                                                    )\n",
    "                df_initial_dim_products.write.mode(\"overwrite\").parquet(dim_products_output_path)\n",
    "                print(f\"Initial dim_products (SCD Type 2) loaded to {dim_products_output_path}.\")\n",
    "            else:\n",
    "                raise e # Re-raise other unexpected errors\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dim_products: {e}\")\n",
    "        # Re-raise to fail the job if critical\n",
    "\n",
    "    # --- 3. Process Dimension Table (dim_customers) - SCD Type 2 ---\n",
    "    print(\"\\nProcessing dim_customers (SCD Type 2)...\")\n",
    "    try:\n",
    "        df_silver_customers = spark.read.format(\"delta\").load(silver_customers_path)\n",
    "\n",
    "        # Add SCD columns: effective_start_date, effective_end_date, current_flag, hash_value\n",
    "        df_silver_customers_with_hash = df_silver_customers.withColumn(\n",
    "            \"customer_hash\",\n",
    "            sha2(concat_ws(\"||\",\n",
    "                col(\"first_name\"),\n",
    "                col(\"last_name\"),\n",
    "                col(\"email\"),\n",
    "                col(\"registration_date\"), # Include in hash if changes to this should trigger new SCD record\n",
    "                col(\"country\")\n",
    "            ), 256)\n",
    "        ).select(\n",
    "            \"customer_id\",\n",
    "            \"first_name\",\n",
    "            \"last_name\",\n",
    "            \"email\",\n",
    "            \"registration_date\",\n",
    "            \"country\",\n",
    "            \"customer_hash\",\n",
    "            current_timestamp().alias(\"effective_start_date\") # New records start now\n",
    "        )\n",
    "\n",
    "        # Check if the target dimension table exists in GCS\n",
    "        try:\n",
    "            df_current_dim_customers = spark.read.parquet(dim_customers_output_path)\n",
    "            print(\"Existing dim_customers found. Performing SCD Type 2 merge.\")\n",
    "\n",
    "            # Identify new records (not in current dim_customers)\n",
    "            new_customers = df_silver_customers_with_hash.alias(\"new_c\").join(\n",
    "                df_current_dim_customers.filter(col(\"current_flag\") == True).alias(\"current_c\"),\n",
    "                col(\"new_c.customer_id\") == col(\"current_c.customer_id\"),\n",
    "                \"left_anti\"\n",
    "            )\n",
    "\n",
    "            # Identify changed records (existing customer_id but changed attributes/hash)\n",
    "            changed_customers = df_silver_customers_with_hash.alias(\"new_c\").join(\n",
    "                df_current_dim_customers.filter(col(\"current_flag\") == True).alias(\"current_c\"),\n",
    "                col(\"new_c.customer_id\") == col(\"current_c.customer_id\"),\n",
    "                \"inner\"\n",
    "            ).where(col(\"new_c.customer_hash\") != col(\"current_c.customer_hash\")) \\\n",
    "            .select(col(\"new_c.*\"))\n",
    "\n",
    "            # Mark old versions of changed customers as expired\n",
    "            expired_customers = df_current_dim_customers.alias(\"current_c\").join(\n",
    "                changed_customers.alias(\"changed_c\"),\n",
    "                (col(\"current_c.customer_id\") == col(\"changed_c.customer_id\")) & (col(\"current_c.current_flag\") == True),\n",
    "                \"inner\"\n",
    "            ).withColumn(\"effective_end_date\", col(\"changed_c.effective_start_date\") - expr(\"INTERVAL 1 DAY\")) \\\n",
    "            .withColumn(\"current_flag\", lit(False)) \\\n",
    "            .select(df_current_dim_customers.columns)\n",
    "\n",
    "\n",
    "            # Combine all pieces for the new dimension table state\n",
    "            current_active_unchanged = df_current_dim_customers.filter(col(\"current_flag\") == True).alias(\"c_curr\") \\\n",
    "                .join(changed_customers.alias(\"c_chg\"), col(\"c_curr.customer_id\") == col(\"c_chg.customer_id\"), \"left_anti\") \\\n",
    "                .select(col(\"c_curr.*\"))\n",
    "\n",
    "            historical_records = df_current_dim_customers.filter(col(\"current_flag\") == False)\n",
    "\n",
    "            new_and_updated_versions = df_silver_customers_with_hash.withColumn(\"effective_end_date\", lit(None).cast(DateType())) \\\n",
    "                                                            .withColumn(\"current_flag\", lit(True).cast(BooleanType())) \\\n",
    "                                                            .select(df_current_dim_customers.columns)\n",
    "\n",
    "\n",
    "            df_final_dim_customers = historical_records.union(expired_customers) \\\n",
    "                                                    .union(current_active_unchanged) \\\n",
    "                                                    .union(new_and_updated_versions)\n",
    "\n",
    "            df_final_dim_customers.write.mode(\"overwrite\").parquet(dim_customers_output_path)\n",
    "            print(f\"dim_customers (SCD Type 2) updated to {dim_customers_output_path}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # If table doesn't exist, this is the first run\n",
    "            if \"Path does not exist\" in str(e) or \"IllegalArgumentException: Path does not exist\" in str(e):\n",
    "                print(\"dim_customers not found. Initial load for SCD Type 2.\")\n",
    "                df_initial_dim_customers = df_silver_customers_with_hash.withColumn(\"effective_end_date\", lit(None).cast(DateType())) \\\n",
    "                                                                    .withColumn(\"current_flag\", lit(True).cast(BooleanType())) \\\n",
    "                                                                    .select(\n",
    "                                                                        \"customer_id\",\n",
    "                                                                        \"first_name\",\n",
    "                                                                        \"last_name\",\n",
    "                                                                        \"email\",\n",
    "                                                                        \"registration_date\",\n",
    "                                                                        \"country\",\n",
    "                                                                        \"customer_hash\",\n",
    "                                                                        \"effective_start_date\",\n",
    "                                                                        \"effective_end_date\",\n",
    "                                                                        \"current_flag\"\n",
    "                                                                    )\n",
    "                df_initial_dim_customers.write.mode(\"overwrite\").parquet(dim_customers_output_path)\n",
    "                print(f\"Initial dim_customers (SCD Type 2) loaded to {dim_customers_output_path}.\")\n",
    "            else:\n",
    "                raise e # Re-raise other unexpected errors\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dim_customers: {e}\")\n",
    "        # Re-raise to fail the job if critical\n",
    "\n",
    "\n",
    "    spark.stop()\n",
    "    print(\"Analytic layer transformations complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"EcomAnalyticLayerTransformations\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # These parameters would typically be passed from Airflow\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: 03_analytic_layer_transformations.py <dlt_storage_path> <analytic_layer_gcs_path>\")\n",
    "        dlt_storage_path = \"gs://batch-processing-de_dlt_storage/\"\n",
    "        analytics_layer_gcs_path = \"gs://batch-processing-de_final_data/dlt/\"\n",
    "        print(f\"using default parameters for the job {dlt_storage_path} {analytics_layer_gcs_path}\")\n",
    "        process_analytic_layer(spark, dlt_storage_path, analytics_layer_gcs_path)\n",
    "\n",
    "\n",
    "    dlt_storage_path = sys.argv[1]      # e.g., 'gs://your-project-id-processed-data/dlt_storage'\n",
    "    analytic_layer_gcs_path = sys.argv[2] # e.g., 'gs://your-project-id-analytic-layer'\n",
    "\n",
    "    process_analytic_layer(spark, dlt_storage_path, analytic_layer_gcs_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
