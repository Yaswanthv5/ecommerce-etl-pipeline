{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark_scripts/03_analytic_layer_transformations.py\n",
    "# This script reads from DLT-generated Silver Delta tables, applies\n",
    "# analytic transformations including CDC/SCD logic, and writes\n",
    "# the final analytical tables to GCS (S3 equivalent) as Parquet.\n",
    "\n",
    "import sys\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lit,\n",
    "    current_timestamp,\n",
    "    date_format,\n",
    "    to_date,\n",
    "    sha2,\n",
    "    concat_ws,\n",
    "    when,\n",
    "    max,\n",
    "    min,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def process_analytic_layer(\n",
    "    # spark: SparkSession,\n",
    "    dlt_storage_path: str,  # Base path of DLT pipeline storage (contains /tables)\n",
    "    analytic_layer_gcs_path_f: str,  # Base path for writing final analytical tables to GCS\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads silver Delta tables, applies analytic transformations and CDC/SCD logic,\n",
    "    and writes to the analytic layer in GCS (Parquet).\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The active SparkSession.\n",
    "        dlt_storage_path (str): Base GCS path to the DLT pipeline's storage location.\n",
    "        analytic_layer_gcs_path (str): Base GCS path to write the final analytical Parquet tables.\n",
    "    \"\"\"\n",
    "    print(f\"Starting analytic layer transformations.\")\n",
    "    print(f\"Reading from DLT storage: {dlt_storage_path}\")\n",
    "    print(f\"Writing to analytic layer GCS: {analytic_layer_gcs_path_f}\")\n",
    "\n",
    "    project_ = \"batch_processing\"\n",
    "    schema = \"e_com\"\n",
    "\n",
    "    # Define paths to DLT Silver tables\n",
    "    silver_sales_path = f\"{project_}.{schema}.silver_sales\"\n",
    "    silver_products_path = f\"{project_}.{schema}.silver_products\"\n",
    "    silver_customers_path = f\"{project_}.{schema}.silver_customers\"\n",
    "\n",
    "    # Define paths for final analytic tables in GCS\n",
    "    fact_sales_output_path = f\"{analytic_layer_gcs_path_f}/fact_daily_sales\"\n",
    "    dim_products_output_path = f\"{analytic_layer_gcs_path_f}/dim_products\"\n",
    "    dim_customers_output_path = f\"{analytic_layer_gcs_path_f}/dim_customers\"\n",
    "\n",
    "    # --- 1. Process Fact Table (fact_daily_sales) ---\n",
    "    print(\"\\nProcessing fact_daily_sales...\")\n",
    "    try:\n",
    "        # Read the latest state of silver_sales (which is already incrementally updated by DLT)\n",
    "        df_silver_sales = spark.read.table(\"batch_processing.e_com.silver_sales\")\n",
    "\n",
    "        if df_silver_sales.isEmpty():\n",
    "            print(\"No new data in silver_sales. Skipping fact_daily_sales processing.\")\n",
    "        else:\n",
    "            # Simple transformation for fact table: select relevant columns for analysis\n",
    "            df_fact_sales = df_silver_sales.select(\n",
    "                col(\"order_id\"),\n",
    "                col(\"customer_id\"),\n",
    "                col(\"order_date\"),  # Use this for partitioning\n",
    "                col(\"order_status\"),\n",
    "                col(\"original_total_amount\").cast(DecimalType(38, 4)).alias(\"original_total_amount\"),\n",
    "                col(\"calculated_order_total\").cast(DecimalType(38, 4)).alias(\"calculated_order_total\"),\n",
    "                col(\"total_products_in_order\"),\n",
    "                col(\"total_quantity_in_order\"),\n",
    "                col(\"silver_processed_timestamp\").alias(\n",
    "                    \"audit_load_timestamp\"\n",
    "                ),  # Audit column\n",
    "            )\n",
    "            df_fact_sales = df_fact_sales.withColumn(\n",
    "                \"order_date\", to_date(col(\"order_date\"), \"YYY-MM-DD\")\n",
    "            )\n",
    "\n",
    "            # display(df_fact_sales)\n",
    "\n",
    "            # Write to GCS as Parquet, partitioned by order_date\n",
    "            # Using 'append' mode because facts are typically append-only.\n",
    "            # Airflow will then pick up new partitions to load to BigQuery.\n",
    "            df_fact_sales.write.mode(\"append\").partitionBy(\"order_date\").parquet(\n",
    "                fact_sales_output_path\n",
    "            )\n",
    "            print(\n",
    "                f\"fact_daily_sales written to {fact_sales_output_path} (appended, partitioned by order_date).\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing fact_daily_sales: {e}\")\n",
    "        # Re-raise to fail the job if critical\n",
    "\n",
    "    # --- 2. Process Dimension Table (dim_products) - SCD Type 2 ---\n",
    "    print(\"\\nProcessing dim_products (SCD Type 2)...\")\n",
    "    try:\n",
    "        df_silver_products = spark.read.table(silver_products_path)\n",
    "\n",
    "        # Define schema for the target dim_products table (if it doesn't exist)\n",
    "        # This will ensure consistent schema during merge operations\n",
    "        # Add SCD columns: effective_start_date, effective_end_date, current_flag, hash_value\n",
    "        df_silver_products_with_hash = df_silver_products.withColumn(\n",
    "            \"product_hash\",\n",
    "            sha2(\n",
    "                concat_ws(\n",
    "                    \"||\",\n",
    "                    col(\"product_name\"),\n",
    "                    col(\"product_category\"),\n",
    "                    col(\"product_price\"),\n",
    "                ),\n",
    "                256,\n",
    "            ),\n",
    "        ).select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"product_category\",\n",
    "            col(\"product_price\").cast(DecimalType(38, 9)).alias(\"product_price\"),\n",
    "            \"product_hash\",\n",
    "            current_timestamp().alias(\"effective_start_date\"),  # New records start now\n",
    "        )\n",
    "\n",
    "        # Check if the target dimension table exists in GCS\n",
    "        # If it's the first run, create it. Otherwise, perform the SCD merge.\n",
    "        try:\n",
    "            df_current_dim_products = spark.read.parquet(dim_products_output_path)\n",
    "            # display(df_current_dim_products)\n",
    "            print(\"Existing dim_products found. Performing SCD Type 2 merge.\")\n",
    "\n",
    "            # Identify new records and changed records\n",
    "            # New records: in silver but not in current_dim (based on product_id)\n",
    "            # Changed records: in both, but hash value differs\n",
    "\n",
    "            # Find records that are NOT in the current dimension (new products)\n",
    "            new_products = df_silver_products_with_hash.alias(\"new_p\").join(\n",
    "                df_current_dim_products.filter(col(\"current_flag\") == True).alias(\n",
    "                    \"current_p\"\n",
    "                ),  # Only compare with current active records\n",
    "                col(\"new_p.product_id\") == col(\"current_p.product_id\"),\n",
    "                \"left_anti\",  # Get rows from new_p that are not in current_p\n",
    "            )\n",
    "\n",
    "            # Find changed records (existing product_id but changed attributes/hash)\n",
    "            changed_products = (\n",
    "                df_silver_products_with_hash.alias(\"new_p\")\n",
    "                .join(\n",
    "                    df_current_dim_products.filter(col(\"current_flag\") == True).alias(\n",
    "                        \"current_p\"\n",
    "                    ),\n",
    "                    col(\"new_p.product_id\") == col(\"current_p.product_id\"),\n",
    "                    \"inner\",\n",
    "                )\n",
    "                .where(col(\"new_p.product_hash\") != col(\"current_p.product_hash\"))\n",
    "                .select(col(\"new_p.*\"))\n",
    "            )  # Select all columns from the new product for the new version\n",
    "\n",
    "            # Get unchanged records (existing product_id and same hash) to carry them forward\n",
    "            unchanged_products = (\n",
    "                df_silver_products_with_hash.alias(\"new_p\")\n",
    "                .join(\n",
    "                    df_current_dim_products.filter(col(\"current_flag\") == True).alias(\n",
    "                        \"current_p\"\n",
    "                    ),\n",
    "                    (col(\"new_p.product_id\") == col(\"current_p.product_id\"))\n",
    "                    & (col(\"new_p.product_hash\") == col(\"current_p.product_hash\")),\n",
    "                    \"inner\",\n",
    "                )\n",
    "                .select(col(\"current_p.*\"))\n",
    "            )  # Keep the existing current record as is\n",
    "\n",
    "            # Mark old versions of changed products as expired\n",
    "            expired_products = (\n",
    "                df_current_dim_products.alias(\"current_p\")\n",
    "                .join(\n",
    "                    changed_products.alias(\"changed_p\"),\n",
    "                    (col(\"current_p.product_id\") == col(\"changed_p.product_id\"))\n",
    "                    & (col(\"current_p.current_flag\") == True),\n",
    "                    \"inner\",\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"effective_end_date\",\n",
    "                    col(\"changed_p.effective_start_date\") - F.expr(\"INTERVAL 1 DAY\"),\n",
    "                )\n",
    "                .withColumn(\"current_flag\", lit(False))\n",
    "                .select(\n",
    "                    \"current_p.product_id\",\n",
    "                    \"current_p.product_name\",\n",
    "                    \"current_p.product_category\",\n",
    "                    \"current_p.product_price\",\n",
    "                    \"current_p.product_hash\",\n",
    "                    \"current_p.effective_start_date\",\n",
    "                    \"effective_end_date\",\n",
    "                    \"current_flag\",\n",
    "                )\n",
    "            )  # Select original columns to maintain schema\n",
    "            # display(expired_products)\n",
    "\n",
    "            # Combine all pieces for the new dimension table state\n",
    "            # 1. Old records that are now expired\n",
    "            # 2. Old records that are unchanged and still current\n",
    "            # 3. New records (either truly new, or new versions of changed records)\n",
    "\n",
    "            # Get records that were current but are NOT in the changed_products set (these remain active or unchanged)\n",
    "            current_active_unchanged = (\n",
    "                df_current_dim_products.filter(col(\"current_flag\") == True)\n",
    "                .alias(\"c_curr\")\n",
    "                .join(\n",
    "                    changed_products.alias(\"c_chg\"),\n",
    "                    col(\"c_curr.product_id\") == col(\"c_chg.product_id\"),\n",
    "                    \"left_anti\",\n",
    "                )\n",
    "                .select(col(\"c_curr.*\"))\n",
    "            )\n",
    "\n",
    "            # Combine all previous non-current records (historicals)\n",
    "            historical_records = df_current_dim_products.filter(\n",
    "                col(\"current_flag\") == False\n",
    "            )\n",
    "\n",
    "            # New versions of changed products and entirely new products\n",
    "            new_and_updated_versions = (\n",
    "                df_silver_products_with_hash.withColumn(\n",
    "                    \"effective_end_date\", lit(None).cast(DateType())\n",
    "                )\n",
    "                .withColumn(\"current_flag\", lit(True).cast(BooleanType()))\n",
    "                .select(df_current_dim_products.columns)\n",
    "            )\n",
    "\n",
    "            df_final_dim_products = (\n",
    "                historical_records\n",
    "                .select(new_and_updated_versions.columns)\n",
    "                .unionByName(expired_products.select(new_and_updated_versions.columns))\n",
    "                .unionByName(unchanged_products.select(new_and_updated_versions.columns))\n",
    "                .unionByName(new_and_updated_versions)\n",
    "            )\n",
    "\n",
    "            df_final_dim_products.write.mode(\"overwrite\").partitionBy(\n",
    "                \"effective_end_date\"\n",
    "            ).parquet(dim_products_output_path)\n",
    "            print(f\"dim_products (SCD Type 2) updated to {dim_products_output_path}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # If table doesn't exist, this is the first run\n",
    "            if \"Path does not exist\" in str(\n",
    "                e\n",
    "            ) or \"IllegalArgumentException: Path does not exist\" in str(e):\n",
    "                print(\"dim_products not found. Initial load for SCD Type 2.\")\n",
    "                df_initial_dim_products = (\n",
    "                    df_silver_products_with_hash.withColumn(\n",
    "                        \"effective_end_date\", lit(None).cast(DateType())\n",
    "                    )\n",
    "                    .withColumn(\"current_flag\", lit(True).cast(BooleanType()))\n",
    "                    .select(\n",
    "                        \"product_id\",\n",
    "                        \"product_name\",\n",
    "                        \"product_category\",\n",
    "                        col(\"product_price\").cast(DecimalType(38, 9)).alias(\"product_price\"),\n",
    "                        \"product_hash\",\n",
    "                        \"effective_start_date\",\n",
    "                        \"effective_end_date\",\n",
    "                        \"current_flag\",\n",
    "                    )\n",
    "                )\n",
    "                df_initial_dim_products.write.mode(\"overwrite\").partitionBy(\"effective_end_date\").parquet(\n",
    "                    dim_products_output_path\n",
    "                )\n",
    "                print(\n",
    "                    f\"Initial dim_products (SCD Type 2) loaded to {dim_products_output_path}.\"\n",
    "                )\n",
    "            else:\n",
    "                raise e  # Re-raise other unexpected errors\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dim_products: {e}\")\n",
    "        # Re-raise to fail the job if critical\n",
    "\n",
    "    # --- 3. Process Dimension Table (dim_customers) - SCD Type 2 ---\n",
    "    print(\"\\nProcessing dim_customers (SCD Type 2)...\")\n",
    "    try:\n",
    "        df_silver_customers = spark.read.table(silver_customers_path)\n",
    "\n",
    "        # Add SCD columns: effective_start_date, effective_end_date, current_flag, hash_value\n",
    "        df_silver_customers_with_hash = df_silver_customers.withColumn(\n",
    "            \"customer_hash\",\n",
    "            sha2(\n",
    "                concat_ws(\n",
    "                    \"||\",\n",
    "                    col(\"customer_first_name\"),\n",
    "                    col(\"customer_last_name\"),\n",
    "                    col(\"customer_email\"),\n",
    "                    col(\n",
    "                        \"customer_registration_date\"\n",
    "                    ),  # Include in hash if changes to this should trigger new SCD record\n",
    "                    col(\"customer_country\"),\n",
    "                ),\n",
    "                256,\n",
    "            ),\n",
    "        ).select(\n",
    "            \"customer_id\",\n",
    "            col(\"customer_first_name\").alias(\"first_name\"),\n",
    "            col(\"customer_last_name\").alias(\"last_name\"),\n",
    "            col(\"customer_email\").alias(\"email\"),\n",
    "            col(\"customer_registration_date\").alias(\"registration_date\"),\n",
    "            col(\"customer_country\").alias(\"country\"),\n",
    "            \"customer_hash\",\n",
    "            current_timestamp().alias(\"effective_start_date\"),  # New records start now\n",
    "        )\n",
    "\n",
    "        # Check if the target dimension table exists in GCS\n",
    "        try:\n",
    "            df_current_dim_customers = spark.read.parquet(dim_customers_output_path)\n",
    "            # display(df_current_dim_customers)\n",
    "            print(\"Existing dim_customers found. Performing SCD Type 2 merge.\")\n",
    "\n",
    "            # Identify new records (not in current dim_customers)\n",
    "            new_customers = df_silver_customers_with_hash.alias(\"new_c\").join(\n",
    "                df_current_dim_customers.filter(col(\"current_flag\") == True).alias(\n",
    "                    \"current_c\"\n",
    "                ),\n",
    "                col(\"new_c.customer_id\") == col(\"current_c.customer_id\"),\n",
    "                \"left_anti\",\n",
    "            )\n",
    "\n",
    "            # Identify changed records (existing customer_id but changed attributes/hash)\n",
    "            changed_customers = (\n",
    "                df_silver_customers_with_hash.alias(\"new_c\")\n",
    "                .join(\n",
    "                    df_current_dim_customers.filter(col(\"current_flag\") == True).alias(\n",
    "                        \"current_c\"\n",
    "                    ),\n",
    "                    col(\"new_c.customer_id\") == col(\"current_c.customer_id\"),\n",
    "                    \"inner\",\n",
    "                )\n",
    "                .where(col(\"new_c.customer_hash\") != col(\"current_c.customer_hash\"))\n",
    "                .select(col(\"new_c.*\"))\n",
    "            )\n",
    "\n",
    "            # Mark old versions of changed customers as expired\n",
    "            expired_customers = (\n",
    "                df_current_dim_customers.alias(\"current_c\")\n",
    "                .join(\n",
    "                    changed_customers.alias(\"changed_c\"),\n",
    "                    (col(\"current_c.customer_id\") == col(\"changed_c.customer_id\"))\n",
    "                    & (col(\"current_c.current_flag\") == True),\n",
    "                    \"inner\",\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"effective_end_date\",\n",
    "                    col(\"changed_c.effective_start_date\") - F.expr(\"INTERVAL 1 DAY\"),\n",
    "                )\n",
    "                .withColumn(\"current_flag\", lit(False))\n",
    "                .select(\n",
    "                    \"current_c.customer_id\",\n",
    "                    \"current_c.first_name\",\n",
    "                    \"current_c.last_name\",\n",
    "                    \"current_c.email\",\n",
    "                    \"current_c.registration_date\",\n",
    "                    \"current_c.country\",\n",
    "                    \"current_c.customer_hash\",\n",
    "                    \"current_c.effective_start_date\",\n",
    "                    \"effective_end_date\",\n",
    "                    \"current_flag\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Combine all pieces for the new dimension table state\n",
    "            current_active_unchanged = (\n",
    "                df_current_dim_customers.filter(col(\"current_flag\") == True)\n",
    "                .alias(\"c_curr\")\n",
    "                .join(\n",
    "                    changed_customers.alias(\"c_chg\"),\n",
    "                    col(\"c_curr.customer_id\") == col(\"c_chg.customer_id\"),\n",
    "                    \"left_anti\",\n",
    "                )\n",
    "                .select(col(\"c_curr.*\"))\n",
    "            )\n",
    "\n",
    "            historical_records = df_current_dim_customers.filter(\n",
    "                col(\"current_flag\") == False\n",
    "            )\n",
    "\n",
    "            new_and_updated_versions = (\n",
    "                df_silver_customers_with_hash.withColumn(\n",
    "                    \"effective_end_date\", lit(None).cast(DateType())\n",
    "                )\n",
    "                .withColumn(\"current_flag\", lit(True).cast(BooleanType()))\n",
    "                .select(df_current_dim_customers.columns)\n",
    "            )\n",
    "\n",
    "            df_final_dim_customers = (\n",
    "                historical_records.select(new_and_updated_versions.columns)\n",
    "                .unionByName(expired_customers.select(new_and_updated_versions.columns))\n",
    "                .unionByName(current_active_unchanged.select(new_and_updated_versions.columns))\n",
    "                .unionByName(new_and_updated_versions)\n",
    "            )\n",
    "\n",
    "            df_final_dim_customers.write.mode(\"overwrite\").partitionBy(\n",
    "                \"effective_end_date\"\n",
    "            ).parquet(dim_customers_output_path)\n",
    "            print(f\"dim_customers (SCD Type 2) updated to {dim_customers_output_path}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # If table doesn't exist, this is the first run\n",
    "            if \"Path does not exist\" in str(\n",
    "                e\n",
    "            ) or \"IllegalArgumentException: Path does not exist\" in str(e):\n",
    "                print(\"dim_customers not found. Initial load for SCD Type 2.\")\n",
    "                df_initial_dim_customers = (\n",
    "                    df_silver_customers_with_hash.withColumn(\n",
    "                        \"effective_end_date\", lit(None).cast(DateType())\n",
    "                    )\n",
    "                    .withColumn(\"current_flag\", lit(True).cast(BooleanType()))\n",
    "                    .select(\n",
    "                        \"customer_id\",\n",
    "                        \"first_name\",\n",
    "                        \"last_name\",\n",
    "                        \"email\",\n",
    "                        \"registration_date\",\n",
    "                        \"country\",\n",
    "                        \"customer_hash\",\n",
    "                        \"effective_start_date\",\n",
    "                        \"effective_end_date\",\n",
    "                        \"current_flag\",\n",
    "                    )\n",
    "                )\n",
    "                df_initial_dim_customers.write.mode(\"overwrite\").partitionBy(\n",
    "                    \"effective_end_date\"\n",
    "                ).parquet(dim_customers_output_path)\n",
    "                print(\n",
    "                    f\"Initial dim_customers (SCD Type 2) loaded to {dim_customers_output_path}.\"\n",
    "                )\n",
    "            else:\n",
    "                raise e  # Re-raise other unexpected errors\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dim_customers: {e}\")\n",
    "        # Re-raise to fail the job if critical\n",
    "\n",
    "    # spark.stop()\n",
    "    print(\"Analytic layer transformations complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # spark = SparkSession.builder \\\n",
    "    #    .appName(\"EcomAnalyticLayerTransformations\") \\\n",
    "    #    .getOrCreate()\n",
    "\n",
    "    # These parameters would typically be passed from Airflow\n",
    "    # if len(sys.argv) != 3:\n",
    "    print(\n",
    "        \"Usage: 03_analytic_layer_transformations.py <dlt_storage_path> <analytic_layer_gcs_path>\"\n",
    "    )\n",
    "    dlt_storage_path = \"gs://batch-processing-de_dlt_storage/\"\n",
    "    analytics_layer_gcs_path_f = \"gs://batch-processing-de_final_data/dlt/\"\n",
    "    print(\n",
    "        f\"using default parameters for the job {dlt_storage_path} {analytics_layer_gcs_path_f}\"\n",
    "    )\n",
    "    # process_analytic_layer(dlt_storage_path, analytics_layer_gcs_path)\n",
    "\n",
    "    # dlt_storage_path = sys.argv[1]      # e.g., 'gs://your-project-id-processed-data/dlt_storage'\n",
    "    # analytic_layer_gcs_path = sys.argv[2] # e.g., 'gs://your-project-id-analytic-layer'\n",
    "    process_analytic_layer(dlt_storage_path, analytics_layer_gcs_path_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
