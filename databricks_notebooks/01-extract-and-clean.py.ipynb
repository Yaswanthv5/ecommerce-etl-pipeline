{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "776c5e7e-14a9-4be9-a6f2-843230f7ecf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing Orders data...\nCleaned Orders data written to gs://batch-processing-de_stagging_data/orders_cleaned\nProcessing Order Items data...\nCleaned Order Items data written to gs://batch-processing-de_stagging_data/order_items_cleaned\nProcessing Products data...\nCleaned Products data written to gs://batch-processing-de_stagging_data/products_cleaned\nProcessing Customers data...\nCleaned Customers data written to gs://batch-processing-de_stagging_data/customers_cleaned\nAll raw data processed and cleaned to Parquet.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you are running this in a Databricks notebook\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, lit, coalesce, sum, count, current_timestamp\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, DateType\n",
    "\n",
    "# Initialize Spark Session (already available in Databricks notebooks)\n",
    "# spark = SparkSession.builder.appName(\"EcomDataExtraction\").getOrCreate()\n",
    "\n",
    "\n",
    "# Define GCS paths\n",
    "raw_data_path = \"gs://batch-processing-de_raw_data/\"\n",
    "processed_data_path = \"gs://batch-processing-de_stagging_data/\"\n",
    "\n",
    "# --- Ingest and Basic Cleaning for Orders ---\n",
    "print(\"Processing Orders data...\")\n",
    "try:\n",
    "    df_orders_raw = spark.read.csv(f\"{raw_data_path}orders.csv\", header=True, inferSchema=True)\n",
    "    df_orders = df_orders_raw.select(\n",
    "        col(\"order_id\").cast(IntegerType()).alias(\"order_id\"),\n",
    "        col(\"customer_id\").cast(IntegerType()).alias(\"customer_id\"),\n",
    "        to_date(col(\"order_date\"), \"yyyy-MM-dd\").alias(\"order_date\"), # Assuming YYYY-MM-DD\n",
    "        col(\"total_amount\").cast(DoubleType()).alias(\"total_amount\"),\n",
    "        col(\"status\").cast(StringType()).alias(\"order_status\")\n",
    "    ).na.drop(subset=[\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"]) # Drop rows with essential nulls\n",
    "\n",
    "    # Add a processing timestamp\n",
    "    df_orders = df_orders.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "    # df_orders.show()\n",
    "\n",
    "    # Write to processed bucket as Parquet (for efficient reading later)\n",
    "    df_orders.write.mode(\"overwrite\").parquet(f\"{processed_data_path}orders_cleaned\")\n",
    "    print(f\"Cleaned Orders data written to {processed_data_path}orders_cleaned\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing orders: {e}\")\n",
    "    # Consider logging to a dedicated logging service\n",
    "\n",
    "# --- Ingest and Basic Cleaning for Order Items ---\n",
    "print(\"Processing Order Items data...\")\n",
    "try:\n",
    "    df_order_items_raw = spark.read.csv(f\"{raw_data_path}order_items.csv\", header=True, inferSchema=True)\n",
    "    df_order_items = df_order_items_raw.select(\n",
    "        col(\"order_item_id\").cast(IntegerType()).alias(\"order_item_id\"),\n",
    "        col(\"order_id\").cast(IntegerType()).alias(\"order_id\"),\n",
    "        col(\"product_id\").cast(IntegerType()).alias(\"product_id\"),\n",
    "        col(\"quantity\").cast(IntegerType()).alias(\"quantity\"),\n",
    "        col(\"unit_price\").cast(DoubleType()).alias(\"unit_price\")\n",
    "    ).na.drop(subset=[\"order_item_id\", \"order_id\", \"product_id\", \"quantity\", \"unit_price\"])\n",
    "\n",
    "    df_order_items = df_order_items.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "    df_order_items.write.mode(\"overwrite\").parquet(f\"{processed_data_path}order_items_cleaned\")\n",
    "    print(f\"Cleaned Order Items data written to {processed_data_path}order_items_cleaned\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing order items: {e}\")\n",
    "\n",
    "# --- Ingest and Basic Cleaning for Products ---\n",
    "print(\"Processing Products data...\")\n",
    "try:\n",
    "    df_products_raw = spark.read.csv(f\"{raw_data_path}products.csv\", header=True, inferSchema=True)\n",
    "    df_products = df_products_raw.select(\n",
    "        col(\"product_id\").cast(IntegerType()).alias(\"product_id\"),\n",
    "        col(\"product_name\").cast(StringType()).alias(\"product_name\"),\n",
    "        col(\"category\").cast(StringType()).alias(\"product_category\"),\n",
    "        col(\"price\").cast(DoubleType()).alias(\"product_price\")\n",
    "    ).na.drop(subset=[\"product_id\", \"product_name\", \"product_price\"])\n",
    "\n",
    "    df_products = df_products.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "    df_products.write.mode(\"overwrite\").parquet(f\"{processed_data_path}products_cleaned\")\n",
    "    print(f\"Cleaned Products data written to {processed_data_path}products_cleaned\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing products: {e}\")\n",
    "\n",
    "# --- Ingest and Basic Cleaning for Customers ---\n",
    "print(\"Processing Customers data...\")\n",
    "try:\n",
    "    df_customers_raw = spark.read.csv(f\"{raw_data_path}customers.csv\", header=True, inferSchema=True)\n",
    "    df_customers = df_customers_raw.select(\n",
    "        col(\"customer_id\").cast(IntegerType()).alias(\"customer_id\"),\n",
    "        col(\"first_name\").cast(StringType()).alias(\"first_name\"),\n",
    "        col(\"last_name\").cast(StringType()).alias(\"last_name\"),\n",
    "        col(\"email\").cast(StringType()).alias(\"email\"),\n",
    "        to_date(col(\"registration_date\"), \"yyyy-MM-dd\").alias(\"registration_date\"),\n",
    "        col(\"country\").cast(StringType()).alias(\"country\")\n",
    "    ).na.drop(subset=[\"customer_id\", \"email\", \"registration_date\"])\n",
    "\n",
    "    df_customers = df_customers.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "    df_customers.write.mode(\"overwrite\").parquet(f\"{processed_data_path}customers_cleaned\")\n",
    "    print(f\"Cleaned Customers data written to {processed_data_path}customers_cleaned\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing customers: {e}\")\n",
    "\n",
    "# spark.stop() # No need to stop in Databricks notebooks'''\n",
    "print(\"All raw data processed and cleaned to Parquet.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-extract-and-clean.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
