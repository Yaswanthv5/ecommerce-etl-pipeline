{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dlt_pipelines/ecom_dlt_pipeline.py\n",
    "# This script is intended to be run as a Delta Live Tables pipeline in Databricks.\n",
    "# It defines the Bronze (cleaned) and Silver (transformed) layers.\n",
    "\n",
    "import dlt\n",
    "from pyspark.sql.functions import col, to_date, lit, coalesce, sum, count, current_timestamp, sha2, concat_ws, expr\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, DateType, BooleanType\n",
    "\n",
    "# Define GCS paths for raw data using DLT's cloud_files for Auto Loader.\n",
    "# This path is relative to the DLT pipeline's storage location unless specified as absolute.\n",
    "# It's best practice to use an absolute GCS path here if raw data is outside the DLT storage.\n",
    "RAW_DATA_GCS_ROOT = \"gs://batch-processing-de_raw_data/\" # Make sure to update this GCS bucket\n",
    "\n",
    "# --- Bronze Layer: Ingest and Cleanse Raw Data ---\n",
    "print(\"Starting to create the Bronze layer...\")\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Raw orders data, incrementally loaded from GCS using Auto Loader. Bronze layer.\",\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    # schema for Auto Loader: DLT will manage it, but schema hint can be useful.\n",
    "    # For more complex schemas, consider a specific schema definition for cloudFiles\n",
    ")\n",
    "def bronze_orders():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{RAW_DATA_GCS_ROOT}_schemas/bronze_orders\") # Auto Loader schema inference checkpoint\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"{RAW_DATA_GCS_ROOT}orders/\")\n",
    "        .select(\n",
    "            col(\"order_id\").cast(IntegerType()).alias(\"order_id\"),\n",
    "            col(\"customer_id\").cast(IntegerType()).alias(\"customer_id\"),\n",
    "            # Handle potential date parsing errors\n",
    "            to_date(col(\"order_date\"), \"yyyy-MM-dd\").alias(\"order_date\"),\n",
    "            col(\"total_amount\").cast(DoubleType()).alias(\"total_amount\"),\n",
    "            col(\"status\").cast(StringType()).alias(\"order_status\"),\n",
    "            current_timestamp().alias(\"bronze_ingestion_timestamp\")\n",
    "        )\n",
    "        .filter(col(\"order_id\").isNotNull() & col(\"customer_id\").isNotNull() & col(\"order_date\").isNotNull() & col(\"total_amount\").isNotNull())\n",
    "        .withWatermark(\"bronze_ingestion_timestamp\", \"10 minutes\")\n",
    "    )\n",
    "\n",
    "print(\"created---table----\")\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Raw order items data, incrementally loaded from GCS using Auto Loader. Bronze layer.\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def bronze_order_items():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{RAW_DATA_GCS_ROOT}_schemas/bronze_order_items\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"{RAW_DATA_GCS_ROOT}order_items/\")\n",
    "        .select(\n",
    "            col(\"order_item_id\").cast(IntegerType()).alias(\"order_item_id\"),\n",
    "            col(\"order_id\").cast(IntegerType()).alias(\"order_id\"),\n",
    "            col(\"product_id\").cast(IntegerType()).alias(\"product_id\"),\n",
    "            col(\"quantity\").cast(IntegerType()).alias(\"quantity\"),\n",
    "            col(\"unit_price\").cast(DoubleType()).alias(\"unit_price\"),\n",
    "            current_timestamp().alias(\"bronze_ingestion_timestamp\")\n",
    "        )\n",
    "        .filter(col(\"order_item_id\").isNotNull() & col(\"order_id\").isNotNull() & col(\"product_id\").isNotNull())\n",
    "        .withWatermark(\"bronze_ingestion_timestamp\", \"10 minutes\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Raw products data, incrementally loaded from GCS using Auto Loader. Bronze layer.\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def bronze_products():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{RAW_DATA_GCS_ROOT}_schemas/bronze_products\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"{RAW_DATA_GCS_ROOT}products/\")\n",
    "        .select(\n",
    "            col(\"product_id\").cast(IntegerType()).alias(\"product_id\"),\n",
    "            col(\"product_name\").cast(StringType()).alias(\"product_name\"),\n",
    "            col(\"category\").cast(StringType()).alias(\"product_category\"),\n",
    "            col(\"price\").cast(DoubleType()).alias(\"product_price\"),\n",
    "            current_timestamp().alias(\"bronze_ingestion_timestamp\")\n",
    "        )\n",
    "        .filter(col(\"product_id\").isNotNull() & col(\"product_name\").isNotNull() & col(\"price\").isNotNull())\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Raw customers data, incrementally loaded from GCS using Auto Loader. Bronze layer.\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{RAW_DATA_GCS_ROOT}_schemas/bronze_customers\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"{RAW_DATA_GCS_ROOT}customers/\")\n",
    "        .select(\n",
    "            col(\"customer_id\").cast(IntegerType()).alias(\"customer_id\"),\n",
    "            col(\"first_name\").cast(StringType()).alias(\"first_name\"),\n",
    "            col(\"last_name\").cast(StringType()).alias(\"last_name\"),\n",
    "            col(\"email\").cast(StringType()).alias(\"email\"),\n",
    "            to_date(col(\"registration_date\"), \"yyyy-MM-dd\").alias(\"registration_date\"),\n",
    "            col(\"country\").cast(StringType()).alias(\"country\"),\n",
    "            current_timestamp().alias(\"bronze_ingestion_timestamp\")\n",
    "        )\n",
    "        .filter(col(\"customer_id\").isNotNull() & col(\"email\").isNotNull())\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Silver Layer: Transform and Enrich Data ---\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Silver sales fact table, joined with product and customer details. Partitioned by order_date.\",\n",
    "    table_properties={\"quality\": \"silver\"},\n",
    "    partition_cols=[\"order_date\"] # Optimize for date-based queries\n",
    ")\n",
    "@dlt.expect(\"order_total_accuracy\", \"abs(original_total_amount - calculated_order_total) < 0.01\")\n",
    "@dlt.expect_or_drop(\"valid_sales_record\", \"order_id IS NOT NULL AND customer_id IS NOT NULL AND order_date IS NOT NULL\")\n",
    "def silver_sales():\n",
    "    # Read from bronze tables. Use `dlt.read` for batch, `dlt.read_stream` for streaming.\n",
    "    # For facts, typically streaming. For dimensions, often batch unless changes are frequent and critical.\n",
    "    orders = dlt.read(\"bronze_orders\")\n",
    "    order_items = dlt.read(\"bronze_order_items\")\n",
    "    products = dlt.read(\"bronze_products\") # Read as batch, assuming product changes are less frequent for simplicity.\n",
    "    customers = dlt.read(\"bronze_customers\") # Read as batch\n",
    "\n",
    "    # Join Order Items with Products and Orders\n",
    "    sales_details = order_items.alias(\"oi\") \\\n",
    "        .join(orders.alias(\"o\"), col(\"oi.order_id\") == col(\"o.order_id\"), \"inner\") \\\n",
    "        .join(products.alias(\"p\"), col(\"oi.product_id\") == col(\"p.product_id\"), \"inner\") \\\n",
    "        .withColumn(\"line_item_total\", col(\"oi.quantity\") * col(\"oi.unit_price\"))\n",
    "\n",
    "    # Aggregate sales by order and perform basic calculations\n",
    "    orders_enriched = sales_details.groupBy(\n",
    "        \"o.order_id\", \"o.customer_id\", \"o.order_date\", \"o.order_status\", \"o.total_amount\"\n",
    "    ).agg(\n",
    "        sum(\"line_item_total\").alias(\"calculated_order_total\"),\n",
    "        count(\"oi.product_id\").alias(\"total_products_in_order\"),\n",
    "        sum(\"oi.quantity\").alias(\"total_quantity_in_order\")\n",
    "    )\n",
    "\n",
    "    # Join with Customer data to enrich sales records\n",
    "    final_silver_sales = orders_enriched.alias(\"ose\") \\\n",
    "        .join(customers.alias(\"c\"), col(\"ose.customer_id\") == col(\"c.customer_id\"), \"inner\") \\\n",
    "        .select(\n",
    "            col(\"ose.order_id\"),\n",
    "            col(\"ose.customer_id\"),\n",
    "            col(\"c.first_name\"),\n",
    "            col(\"c.last_name\"),\n",
    "            col(\"c.email\"),\n",
    "            col(\"c.country\"),\n",
    "            col(\"c.registration_date\").alias(\"customer_registration_date\"),\n",
    "            col(\"ose.order_date\"),\n",
    "            col(\"ose.order_status\"),\n",
    "            col(\"ose.total_amount\").alias(\"original_total_amount\"),\n",
    "            col(\"ose.calculated_order_total\"),\n",
    "            col(\"ose.total_products_in_order\"),\n",
    "            col(\"ose.total_quantity_in_order\"),\n",
    "            current_timestamp().alias(\"silver_processed_timestamp\")\n",
    "            # For CDC, you might add a hash of relevant columns to detect changes\n",
    "            # sha2(concat_ws(\"||\", *[c for c in ose.columns if c not in [\"order_id\", \"customer_id\", \"order_date\"]]), 256).alias(\"record_hash\")\n",
    "        )\n",
    "    return final_silver_sales\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Silver products dimension table. SCD Type 1 (latest state) for simplicity.\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_product_record\", \"product_id IS NOT NULL AND product_name IS NOT NULL\")\n",
    "def silver_products():\n",
    "    return (\n",
    "        dlt.read(\"bronze_products\") # Reading as batch for dimension table\n",
    "        .select(\n",
    "            col(\"product_id\"),\n",
    "            col(\"product_name\"),\n",
    "            col(\"product_category\"),\n",
    "            col(\"product_price\"),\n",
    "            current_timestamp().alias(\"silver_processed_timestamp\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Silver customers dimension table. SCD Type 1 (latest state) for simplicity.\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_customer_record\", \"customer_id IS NOT NULL AND customer_email IS NOT NULL\")\n",
    "def silver_customers():\n",
    "    return (\n",
    "        dlt.read(\"bronze_customers\") # Reading as batch for dimension table\n",
    "        .select(\n",
    "            col(\"customer_id\"),\n",
    "            col(\"first_name\").alias(\"customer_first_name\"),\n",
    "            col(\"last_name\").alias(\"customer_last_name\"),\n",
    "            col(\"email\").alias(\"customer_email\"),\n",
    "            col(\"registration_date\").alias(\"customer_registration_date\"),\n",
    "            col(\"country\").alias(\"customer_country\"),\n",
    "            current_timestamp().alias(\"silver_processed_timestamp\")\n",
    "        )\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
